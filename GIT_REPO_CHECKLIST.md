# Git Repository Checklist: What to Push to Public Repo

**Purpose:** Guide for sharing code and results with team members while protecting sensitive data.

---

## âœ… INCLUDE (Push to Repo)

### ğŸ“ Core Documentation
- âœ… `README.md` - Main project documentation
- âœ… `COMPREHENSIVE_RESULTS_REPORT.md` - Complete results report
- âœ… `PROJECT_SUMMARY.md` - Project overview
- âœ… `IMPLEMENTATION_NOTES.md` - Lessons learned
- âœ… `DOCUMENTATION_INDEX.md` - Navigation guide
- âœ… `FINAL_RESULTS_SUMMARY.md` - Executive summary
- âœ… `MLFLOW_TRACKING_GUIDE.md` - MLflow guide
- âœ… `MODEL_COMPARISON_SUMMARY.md` - Historical context
- âœ… `SYSTEM_OVERVIEW.md` - Architecture overview
- âœ… `ANALYSIS_MECHANISMS_EXPLAINED.md` - Technical deep-dive
- âœ… `QUICK_START_GUIDE.md` - Quick start guide
- âœ… `GIT_REPO_CHECKLIST.md` - This file

### ğŸ’» Core Python Scripts
- âœ… `validate_against_human.py` - Human score extraction
- âœ… `run_models_on_gold_standard.py` - Model execution
- âœ… `compare_models_to_human.py` - Statistical comparison
- âœ… `generate_team_report.py` - Report generation
- âœ… `llm_analyzer.py` - LLM API integration
- âœ… `models.py` - RoBERTa models
- âœ… `scale.py` - Valence scaling
- âœ… `parse_transcripts_docx.py` - Transcript parsing
- âœ… `utils.py` - Utility functions
- âœ… `compare_all_models.py` - Historical comparison script
- âœ… `compare_runs_consistency.py` - Run consistency analysis
- âœ… `build.py` - Transcript fetching utilities
- âœ… `config.py` - Configuration (if no secrets)

### ğŸ“Š Results and Outputs (Summary Data Only)
- âœ… `validation_results/human_scores_cleaned.csv` - Aggregated human scores (anonymized)
- âœ… `validation_results/human_metrics_summary.csv` - Summary statistics
- âœ… `validation_results/inter_rater_reliability.csv` - Human agreement metrics
- âœ… `validation_results/missing_data_report.csv` - Missing data analysis
- âœ… `validation_results/data_quality_notes.json` - Data quality notes
- âœ… `validation_results/*.png` - Human score visualizations
- âœ… `model_comparison_results/comparison_summary_*.json` - Best methods summary
- âœ… `model_comparison_results/model_vs_human_metrics_*.csv` - Statistical metrics
- âœ… `model_comparison_results/plots/*.png` - Visualizations (scatter plots, heatmaps)
- âœ… `team_report/team_report_*.md` - Team reports
- âœ… `team_report/summary_stats_*.json` - Summary statistics
- âœ… `model_scores_gold_standard/run_*/model_scores_*.csv` - Summary model scores (CSV only, not detailed JSON)

### ğŸ“¦ Configuration Files
- âœ… `requirements.txt` - Python dependencies
- âœ… `.gitignore` - Git ignore rules

### ğŸ“‹ Templates and Examples
- âœ… `human_scores_template.csv` - Template for human scoring

### ğŸ“ˆ Sample Data (If Appropriate)
- âœ… `transcript_corpus_v2.csv` - Sample transcript corpus (if not sensitive)
- âœ… `transcript_corpus.csv` - Sample corpus (if not sensitive)

---

## âŒ EXCLUDE (Do NOT Push)

### ğŸ” Sensitive Data
- âŒ `.env` files - API keys and secrets (already in .gitignore)
- âŒ `gold_standard.xlsx` - **Contains human evaluator names and potentially sensitive data**
  - **Alternative:** Create anonymized version or exclude
- âŒ `transcripts/Transcripts.docx` - **May contain copyrighted content**
  - **Alternative:** Include sample transcripts only, or exclude entirely
- âŒ `transcripts/history/*.txt` - Individual transcript files (large, potentially copyrighted)
- âŒ Any files with API keys, passwords, or personal information

### ğŸ’¾ Large Files and Caches
- âŒ `mlruns/` - MLflow tracking database (can be large, regenerated)
- âŒ `__pycache__/` - Python bytecode (already in .gitignore)
- âŒ `*.pyc`, `*.pyo`, `*.pyd` - Compiled Python files (already in .gitignore)
- âŒ `subtitles/` - Temporary subtitle files (already in .gitignore)
- âŒ `results/` - Old results (already in .gitignore)
- âŒ `experiments/` - Experimental files (already in .gitignore)

### ğŸ—‚ï¸ Detailed Model Outputs (Too Large)
- âŒ `model_scores_gold_standard/run_*/model_scores_detailed_*.json` - **Very large files with full rationales**
  - **Alternative:** Include only the CSV summaries, exclude detailed JSON
- âŒ `model_comparison_results/merged_human_model_scores_*.csv` - **Contains full individual scores**
  - **Alternative:** Include only summary metrics, not individual video scores

### ğŸ› ï¸ Development Files
- âŒ `*.code-workspace` - VSCode workspace files (already in .gitignore)
- âŒ `.vscode/` - VSCode settings (already in .gitignore)
- âŒ `testenv/` - Virtual environment (already in .gitignore)
- âŒ `venv/` - Virtual environment (already in .gitignore)
- âŒ `test_*.py` - Test files (optional, can include if useful)
- âŒ `app.py` - Flask app (if contains test code only)

### ğŸ“± Extension/Experimental Code
- âŒ `extension/` - Chrome extension (if not core to research)
- âŒ `experiments/` - Experimental code (already in .gitignore)
- âŒ `hist_*.py` - History extraction scripts (if not core)

### ğŸ“„ Office Files (Temporary)
- âŒ `~$*.xlsx` - Excel temporary files (now in .gitignore)
- âŒ `~$*.docx` - Word temporary files (now in .gitignore)
- âŒ `*.xlsx` in `comparison_results/` - Large Excel files (CSV is better)

---

## ğŸ”„ RECOMMENDED: Create Anonymized/Sample Versions

### For `gold_standard.xlsx`:
Create `gold_standard_sample.xlsx` with:
- âœ… Sample of 5-10 videos (not all 52)
- âœ… Anonymized evaluator names (EA, JPA, MMM â†’ Evaluator1, Evaluator2, Evaluator3)
- âœ… Remove any identifying information

### For Transcripts:
- âœ… Include 2-3 sample transcripts in `samples/transcripts/` folder
- âœ… Add note: "Full transcripts available upon request"

### For Model Scores:
- âœ… Include summary CSVs only
- âœ… Exclude detailed JSON with full rationales (too large)
- âœ… Add note: "Detailed results available upon request"

---

## ğŸ“ Pre-Push Checklist

Before pushing to public repo:

- [ ] Review all files for API keys, passwords, personal info
- [ ] Remove or anonymize `gold_standard.xlsx` (or exclude)
- [ ] Exclude large detailed JSON files
- [ ] Exclude individual transcript files
- [ ] Update `.gitignore` with exclusions (âœ… DONE)
- [ ] Test clone in fresh directory to verify nothing sensitive is included
- [ ] Add `LICENSE` file (if applicable)
- [ ] Add `CONTRIBUTING.md` (if team will contribute)
- [ ] Update `README.md` with setup instructions (without API keys)

---

## ğŸ¯ Recommended Repository Structure

```
peace-research-ai-validation/
â”œâ”€â”€ README.md                          # Main documentation
â”œâ”€â”€ requirements.txt                   # Dependencies
â”œâ”€â”€ .gitignore                         # Exclusions
â”œâ”€â”€ LICENSE                            # License (if applicable)
â”‚
â”œâ”€â”€ docs/                              # Documentation
â”‚   â”œâ”€â”€ COMPREHENSIVE_RESULTS_REPORT.md
â”‚   â”œâ”€â”€ PROJECT_SUMMARY.md
â”‚   â”œâ”€â”€ IMPLEMENTATION_NOTES.md
â”‚   â””â”€â”€ ...
â”‚
â”œâ”€â”€ src/                               # Core scripts
â”‚   â”œâ”€â”€ validate_against_human.py
â”‚   â”œâ”€â”€ run_models_on_gold_standard.py
â”‚   â”œâ”€â”€ compare_models_to_human.py
â”‚   â”œâ”€â”€ llm_analyzer.py
â”‚   â”œâ”€â”€ models.py
â”‚   â””â”€â”€ ...
â”‚
â”œâ”€â”€ data/                              # Sample data
â”‚   â”œâ”€â”€ samples/
â”‚   â”‚   â””â”€â”€ transcripts/              # 2-3 sample transcripts
â”‚   â””â”€â”€ gold_standard_sample.xlsx     # Anonymized sample
â”‚
â”œâ”€â”€ results/                           # Summary results only
â”‚   â”œâ”€â”€ validation_results/           # Human score summaries
â”‚   â”œâ”€â”€ model_comparison_results/     # Metrics and plots
â”‚   â””â”€â”€ team_report/                  # Reports
â”‚
â””â”€â”€ templates/
    â””â”€â”€ human_scores_template.csv
```

---

## ğŸ” Security Review Commands

Before making repo public, run these commands:

```bash
# Search for API keys
grep -r "api_key" . --exclude-dir=.git --exclude="*.md" --exclude="*.txt"
grep -r "API_KEY" . --exclude-dir=.git --exclude="*.md" --exclude="*.txt"
grep -r "sk-" . --exclude-dir=.git --exclude="*.md" --exclude="*.txt"  # OpenAI keys

# Check for large files
find . -type f -size +10M -not -path "./.git/*"

# Review what will be committed
git status
git diff --cached  # If staging files
```

---

## ğŸ“§ For Team Members

**What they'll get:**
- âœ… All code to run the analysis
- âœ… Documentation and reports
- âœ… Summary statistics and visualizations
- âœ… Sample data to understand structure

**What they'll need:**
- ğŸ”‘ Their own API keys (OpenAI, Gemini)
- ğŸ“Š Access to full gold standard (if needed, share separately)
- ğŸ“ Full transcripts (if needed, share separately)

**Setup instructions:**
1. Clone repo
2. Install dependencies: `pip install -r requirements.txt`
3. Set up `.env` file with their own API keys:
   ```
   OPENAI_API_KEY=their_key_here
   GEMINI_API_KEY=their_key_here
   ```
4. Run scripts as documented in README

---

## ğŸš€ Quick Commands

```bash
# Review what will be committed
git status

# Check for large files
find . -type f -size +5M -not -path "./.git/*"

# Check for potential secrets
grep -r "api_key\|API_KEY\|password\|secret" . --exclude-dir=.git --exclude="*.md" --exclude="*.txt"

# See what files are tracked/untracked
git ls-files
git ls-files --others --excluded --exclude-standard
```

---

**Last Updated:** November 18, 2024

