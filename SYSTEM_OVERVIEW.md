# System Overview: Model Comparison Framework

## ğŸ“ Architecture Diagram

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                         TRANSCRIPT CORPUS                               â”‚
â”‚                     (transcript_corpus_v2.csv)                          â”‚
â”‚                  49+ transcripts from YouTube videos                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                 â”‚
                                 â”‚ Input
                                 â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    COMPARE ALL MODELS SCRIPT                            â”‚
â”‚                    (compare_all_models.py)                              â”‚
â”‚                                                                         â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚   Method 1   â”‚  â”‚   Method 2   â”‚  â”‚   Method 3   â”‚  â”‚  Method 4  â”‚   â”‚
â”‚  â”‚   RoBERTa    â”‚  â”‚   RoBERTa    â”‚  â”‚    LLM V1    â”‚  â”‚  LLM V3    â”‚   â”‚
â”‚  â”‚    Plain     â”‚  â”‚   Valence    â”‚  â”‚  (Plain)     â”‚  â”‚  (Hybrid)  â”‚   â”‚
â”‚  â”‚              â”‚  â”‚              â”‚  â”‚              â”‚  â”‚            â”‚   â”‚
â”‚  â”‚  models.py   â”‚  â”‚  scale.py    â”‚  â”‚llm_analyzer  â”‚  â”‚llm_analyzerâ”‚   â”‚
â”‚  â”‚              â”‚  â”‚              â”‚  â”‚   .py        â”‚  â”‚   .py      â”‚   â”‚
â”‚  â”‚  FREE âœ“      â”‚  â”‚  FREE âœ“     â”‚  â”‚  PAID $      â”‚  â”‚  PAID $$   â”‚   â”‚
â”‚  â”‚  Fast âš¡âš¡  â”‚  â”‚ Fast âš¡âš¡   â”‚  â”‚  Slow âš¡    â”‚  â”‚  Slow âš¡   â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                                                                         â”‚
â”‚  Each method outputs score on 0-5 scale                                 â”‚
â”‚  (0 = contemptuous/negative, 5 = compassionate/positive)                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                 â”‚
                                 â”‚ Output
                                 â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                        COMPARISON RESULTS                               â”‚
â”‚                    (comparison_results/ folder)                         â”‚
â”‚                                                                         â”‚
â”‚  ğŸ“Š score_comparison_TIMESTAMP.xlsx  (Color-coded Excel)                â”‚
â”‚  ğŸ“‹ score_comparison_TIMESTAMP.csv   (Raw data)                         â”‚
â”‚  ğŸ” detailed_comparison_TIMESTAMP.json (Full results with rationales)   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                 â”‚
                                 â”‚ Compare to
                                 â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                         HUMAN GROUND TRUTH                              â”‚
â”‚                         (human_scores.csv)                              â”‚
â”‚              Manually coded scores by human researchers                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                 â”‚
                                 â”‚ Validate
                                 â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    VALIDATION SCRIPT                                   â”‚
â”‚                 (validate_against_human.py)                            â”‚
â”‚                                                                        â”‚
â”‚  Calculates for each method:                                           â”‚
â”‚  â€¢ MAE (Mean Absolute Error)                                           â”‚
â”‚  â€¢ Correlation (Pearson, Spearman)                                     â”‚
â”‚  â€¢ Agreement (% within 1 point)                                        â”‚
â”‚  â€¢ Bias (systematic over/under scoring)                                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                 â”‚
                                 â”‚ Output
                                 â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                        VALIDATION RESULTS                               â”‚
â”‚                    (validation_results/ folder)                         â”‚
â”‚                                                                         â”‚
â”‚  ğŸ“Š validation_metrics.csv        (Comparison table)                    â”‚
â”‚  ğŸ“ˆ scatter_comparison.png        (Correlation plots)                   â”‚
â”‚  ğŸ“ˆ error_distribution.png        (Bias analysis)                       â”‚
â”‚  ğŸ“ˆ bland_altman.png              (Agreement analysis)                  â”‚
â”‚  ğŸ† BEST METHOD IDENTIFIED!                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ”„ Workflow Diagram

```
START
  â”‚
  â”œâ”€â–º 1. Verify Setup
  â”‚     python test_setup.py
  â”‚     â””â”€â–º âœ… All dependencies installed?
  â”‚         â””â”€â–º âŒ No â†’ Install packages, set API keys
  â”‚         â””â”€â–º âœ… Yes â†’ Continue
  â”‚
  â”œâ”€â–º 2. Run Quick Test
  â”‚     python compare_all_models.py --quick-test
  â”‚     â””â”€â–º Analyzes 2 transcripts with all 4 methods
  â”‚     â””â”€â–º Output: Excel file with scores
  â”‚
  â”œâ”€â–º 3. Review Results
  â”‚     Open: comparison_results/score_comparison_*.xlsx
  â”‚     â””â”€â–º Look for agreement/disagreement
  â”‚     â””â”€â–º Understand which methods align
  â”‚
  â”œâ”€â–º 4. Scale Up (Optional)
  â”‚     python compare_all_models.py --num-samples 20
  â”‚     â””â”€â–º Analyze more transcripts
  â”‚     â””â”€â–º âš ï¸  Watch API costs for LLM methods
  â”‚
  â”œâ”€â–º 5. Add Human Scores (Recommended)
  â”‚     â”‚
  â”‚     â”œâ”€â–º 5a. Select sample (20-50 transcripts)
  â”‚     â”‚
  â”‚     â”œâ”€â–º 5b. Code manually using 0-5 scale
  â”‚     â”‚     â””â”€â–º Use human_scores_template.csv
  â”‚     â”‚
  â”‚     â””â”€â–º 5c. Save as human_scores.csv
  â”‚
  â”œâ”€â–º 6. Validate
  â”‚     python validate_against_human.py
  â”‚     â””â”€â–º Identifies best-performing method
  â”‚     â””â”€â–º Generates validation plots
  â”‚
  â””â”€â–º 7. Choose Method & Deploy
        â”‚
        â”œâ”€â–º Option A: Use LLM V3 (best accuracy, highest cost)
        â”œâ”€â–º Option B: Use RoBERTa Valence (good balance, free)
        â”œâ”€â–º Option C: Use ensemble (weighted combination)
        â”‚
        â””â”€â–º Run on full dataset with chosen method
             â””â”€â–º Publish results!

END
```

---

## ğŸ“Š Method Comparison Matrix

|  | RoBERTa Plain | RoBERTa Valence | LLM V1 | LLM V3_FINAL |
|---|:---:|:---:|:---:|:---:|
| **Cost** | Free | Free | $ | $$ |
| **Speed** | âš¡âš¡âš¡ | âš¡âš¡âš¡ | âš¡ | âš¡ |
| **Accuracy** | â­â­ | â­â­â­ | â­â­â­â­ | â­â­â­â­â­ |
| **Interpretability** | â­â­â­ | â­â­â­ | â­â­â­â­â­ | â­â­â­â­â­ |
| **Reproducibility** | 100% | 100% | ~80% | ~85% |
| **API Required** | âŒ No | âŒ No | âœ… Yes | âœ… Yes |
| **Offline Capable** | âœ… Yes | âœ… Yes | âŒ No | âŒ No |
| **Best For** | Bulk analysis | Balanced approach | Deep insight | Research validation |

---

## ğŸ“ File Structure

```
yt_parser/
â”‚
â”œâ”€â”€ ğŸ“œ Core Analysis Scripts
â”‚   â”œâ”€â”€ models.py                 # RoBERTa emotion detection
â”‚   â”œâ”€â”€ scale.py                  # Valence scaling logic
â”‚   â”œâ”€â”€ llm_analyzer.py           # LLM prompts and analysis
â”‚   â”œâ”€â”€ compare_all_models.py     # Main comparison script â­
â”‚   â””â”€â”€ validate_against_human.py # Validation script â­
â”‚
â”œâ”€â”€ ğŸ“š Documentation
â”‚   â”œâ”€â”€ README.md                          # Project overview
â”‚   â”œâ”€â”€ QUICK_START_GUIDE.md              # Step-by-step tutorial â­â­â­
â”‚   â”œâ”€â”€ ANALYSIS_MECHANISMS_EXPLAINED.md   # Technical deep-dive
â”‚   â”œâ”€â”€ MODEL_COMPARISON_SUMMARY.md        # High-level summary
â”‚   â””â”€â”€ SYSTEM_OVERVIEW.md                 # This file (architecture)
â”‚
â”œâ”€â”€ ğŸ“Š Data Files
â”‚   â”œâ”€â”€ transcript_corpus_v2.csv       # Input transcripts
â”‚   â”œâ”€â”€ human_scores_template.csv      # Template for coding
â”‚   â””â”€â”€ human_scores.csv               # Your coded scores (create this)
â”‚
â”œâ”€â”€ ğŸ“¦ Output Directories
â”‚   â”œâ”€â”€ comparison_results/            # Comparison outputs
â”‚   â”‚   â”œâ”€â”€ score_comparison_*.xlsx
â”‚   â”‚   â”œâ”€â”€ score_comparison_*.csv
â”‚   â”‚   â””â”€â”€ detailed_comparison_*.json
â”‚   â”‚
â”‚   â””â”€â”€ validation_results/            # Validation outputs
â”‚       â”œâ”€â”€ validation_metrics.csv
â”‚       â”œâ”€â”€ scatter_comparison.png
â”‚       â”œâ”€â”€ error_distribution.png
â”‚       â””â”€â”€ bland_altman.png
â”‚
â”œâ”€â”€ ğŸ”§ Utilities
â”‚   â”œâ”€â”€ test_setup.py              # Verify installation
â”‚   â”œâ”€â”€ build.py                   # Build transcript corpus
â”‚   â””â”€â”€ requirements.txt           # Dependencies
â”‚
â””â”€â”€ ğŸŒ Extension (separate component)
    â””â”€â”€ extension/                 # Chrome extension files
```

---

## ğŸ¯ Quick Decision Tree

### "Which method should I use?"

```
START: What's your priority?
â”‚
â”œâ”€â–º Need it FREE?
â”‚   â””â”€â–º Use RoBERTa Valence âœ“
â”‚       â€¢ Fast, sophisticated, no cost
â”‚       â€¢ Good for large datasets
â”‚
â”œâ”€â–º Need BEST ACCURACY?
â”‚   â””â”€â–º Do you have human validation data?
â”‚       â”œâ”€â–º Yes â†’ Run validation, use best performer
â”‚       â””â”€â–º No â†’ Use LLM V3_FINAL (usually most accurate)
â”‚
â”œâ”€â–º Need EXPLANATIONS?
â”‚   â””â”€â–º Use LLM V1 or V3_FINAL âœ“
â”‚       â€¢ Provides rationales
â”‚       â€¢ Shows reasoning process
â”‚
â”œâ”€â–º Working OFFLINE?
â”‚   â””â”€â–º Use RoBERTa methods âœ“
â”‚       â€¢ No internet required
â”‚       â€¢ Works on local machine
â”‚
â””â”€â–º Not sure? ğŸ¤”
    â””â”€â–º Run compare_all_models.py --quick-test
        â””â”€â–º See results, then decide!
```

---

## ğŸ”‘ Key Concepts

### 0-5 Scoring Scale (Unified Across All Methods)

All methods now output on the same scale for fair comparison:

```
5.0  â”€â”€â”
       â”‚  Highly compassionate, inclusive
4.0  â”€â”€â”¤  Mostly positive, constructive
       â”‚
3.0  â”€â”€â”¤  Neutral, balanced
       â”‚
2.0  â”€â”€â”¤  Somewhat negative, divisive
       â”‚
1.0  â”€â”€â”¤  Clearly contemptuous
       â”‚
0.0  â”€â”€â”˜  Extremely hostile
```

### Method Categories

**Quantitative (Data-Driven):**
- RoBERTa Plain
- RoBERTa Valence

**Qualitative (Reasoning-Based):**
- LLM V1

**Hybrid (Best of Both):**
- LLM V3_FINAL

---

## ğŸ“ Research Implications

### For Peace Research

This framework allows you to:

1. **Compare Outlets**: Do CNN, Fox News score differently?
2. **Track Over Time**: Is media becoming more/less peaceful?
3. **Validate Theories**: Test hypotheses about framing
4. **Scale Analysis**: Code thousands of videos efficiently
5. **Replicate Studies**: Share methodology and code

### Publication Checklist

When publishing research using this framework:

- [ ] Document which method(s) used
- [ ] Report validation metrics (if applicable)
- [ ] Include inter-rater reliability (if multiple coders)
- [ ] Make code and data available
- [ ] Report all method scores in appendix
- [ ] Cite relevant papers (RoBERTa model, Go Emotions dataset)

---

## ğŸš€ Performance Benchmarks

Based on typical usage:

| Task | RoBERTa Plain | RoBERTa Valence | LLM V1 | LLM V3_FINAL |
|------|:---:|:---:|:---:|:---:|
| **Time per transcript** | 2-5 sec | 2-5 sec | 10-20 sec | 12-25 sec |
| **100 transcripts** | 5 min | 5 min | 30 min | 40 min |
| **1000 transcripts** | 45 min | 45 min | 5 hours | 7 hours |
| **Cost for 100** | $0 | $0 | ~$3 | ~$5 |

*Note: LLM times include API latency. Actual times may vary based on network, API rate limits.*

---

## ğŸ”® Future Enhancements

Potential improvements (not yet implemented):

- [ ] Ensemble method (combine multiple methods)
- [ ] Fine-tuned RoBERTa on peace research corpus
- [ ] Support for non-English languages
- [ ] Batch processing with progress bars
- [ ] Web UI for easier interaction
- [ ] Automatic hyperparameter tuning
- [ ] Cache results to avoid recomputation
- [ ] Parallel processing for faster analysis

---

## ğŸ’¬ Common Scenarios

### Scenario 1: "I have 1000 transcripts and no budget"
**Solution**: Use RoBERTa Valence
- Fast (< 1 hour)
- Free
- Decent accuracy
- Can validate on subset with LLM later

### Scenario 2: "I need highest accuracy for academic paper"
**Solution**: Use validation pipeline
1. Run all methods on 50 random samples
2. Code those 50 yourself
3. Run validation
4. Use best performer on full dataset
5. Report all methods' correlations

### Scenario 3: "I want to test if method works before scaling"
**Solution**: Use quick test
1. `python compare_all_models.py --quick-test`
2. Manually check 2 results
3. If reasonable, scale up

### Scenario 4: "I already have human scores for some transcripts"
**Solution**: Start with validation
1. `python validate_against_human.py`
2. See which method matches your coding
3. Use that method going forward

---

## ğŸ†˜ Getting Help

1. **Check Documentation First**:
   - README.md (overview)
   - QUICK_START_GUIDE.md (tutorial)
   - ANALYSIS_MECHANISMS_EXPLAINED.md (technical)

2. **Run Setup Test**:
   ```bash
   python test_setup.py
   ```

3. **Check Output Files**:
   - Logs show detailed progress
   - JSON files have full debug info

4. **Common Issues**: See QUICK_START_GUIDE.md "Troubleshooting" section

---

## ğŸ“‹ Checklist for First-Time Users

- [ ] Clone/download repository
- [ ] Install Python 3.8+
- [ ] Run: `pip install -r requirements.txt`
- [ ] Create `.env` file with API keys (optional for LLM)
- [ ] Run: `python test_setup.py`
- [ ] Run: `python compare_all_models.py --quick-test`
- [ ] Open Excel results and review
- [ ] Read QUICK_START_GUIDE.md for next steps
- [ ] Code 20-30 transcripts manually (optional but recommended)
- [ ] Run validation (if you have human scores)
- [ ] Choose method based on results
- [ ] Scale to full dataset

---

*This framework was designed to be:*
- âœ… **Flexible**: Works with 2 transcripts or 2000
- âœ… **Transparent**: All code is documented and open
- âœ… **Rigorous**: Validation metrics ensure reliability
- âœ… **Practical**: Balances cost, speed, and accuracy
- âœ… **Research-Grade**: Publication-ready methodology

**Happy analyzing!** ğŸ“Šâœ¨

---

*Last Updated: 2025-11-07*

